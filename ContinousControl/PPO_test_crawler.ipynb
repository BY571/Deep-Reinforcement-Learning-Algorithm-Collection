{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import gym\n",
    "import math\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: CrawlerBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 129\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 20\n",
      "        Vector Action descriptions: , , , , , , , , , , , , , , , , , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 12\n",
      "Size of each action: 20\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Crawler_Linux/Crawler.x86_64')\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states_ = env_info.vector_observations\n",
    "state_size = states_.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_shape, layer_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(input_shape, layer_size),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(layer_size,layer_size),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(layer_size, 1))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "      \n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape,layer_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(input_shape, layer_size),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(layer_size,layer_size),\n",
    "                                 nn.ReLU(),\n",
    "                                 )\n",
    "        self.mean = nn.Sequential(nn.Linear(layer_size, output_shape),\n",
    "                                  nn.Tanh())                    # tanh squashed output to the range of -1..1\n",
    "        self.variance =nn.Sequential(nn.Linear(layer_size, output_shape),\n",
    "                                     nn.Softplus())             # log(1 + e^x) has the shape of a smoothed ReLU\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)    \n",
    "        sigma = torch.sqrt(self.variance(x).cpu())\n",
    "        m = Normal(self.mean(x).cpu(), sigma)\n",
    "        actions = m.sample()\n",
    "        actions = torch.clamp(actions, -1, 1) # usually clipping between -1,1 but pendulum env has action range of -2,2\n",
    "\n",
    "        logprobs = m.log_prob(actions) #for the optimization step we create a new distribution based on the new mean and variance - still taking the logprobs based on the old actions!\n",
    "\n",
    "    \n",
    "        return actions, logprobs, m\n",
    "    \n",
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, ppo_epochs, mini_batch_size,\\\n",
    "                 layer_size,lr_a, lr_c, gamma, entropy_beta, clip_grad):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.layer_size = layer_size\n",
    "        self.gamma = gamma\n",
    "        self.entropy_beta = entropy_beta\n",
    "        self.clip_grad = clip_grad\n",
    "        \n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        \n",
    "        self.actor = Actor(state_size, action_size,layer_size).to(device)\n",
    "        self.critic = Critic(state_size,layer_size).to(device)\n",
    "        self.a_optimizer = optim.RMSprop(params = self.actor.parameters(),lr = lr_a)\n",
    "        self.c_optimizer = optim.RMSprop(params = self.critic.parameters(),lr = lr_c)\n",
    "        \n",
    "    def act(self, states):\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            actions, logprobs ,_ = self.actor(torch.from_numpy(states).float().to(device))\n",
    "        self.actor.train()\n",
    "        return actions.cpu().numpy(), logprobs\n",
    "        \n",
    "\n",
    "    def compute_returns(self,rewards_tensor, masks_tensor):\n",
    "        output = []\n",
    "        for rewards, masks in zip(rewards_tensor, masks_tensor):\n",
    "            R = 0 \n",
    "            returns = []\n",
    "            for step in reversed(range(len(rewards))):\n",
    "                R = rewards[step] + self.gamma * R * masks[step]\n",
    "                returns.insert(0, R)\n",
    "            output.append(returns)\n",
    "        output = list(zip(*output))\n",
    "        discounted_rewards = [torch.FloatTensor(i).unsqueeze(1) for i in output]\n",
    "        return torch.cat(discounted_rewards)\n",
    "\n",
    "\n",
    "\n",
    "    def ppo_iter(self, states, actions, log_probs, advantage, discounted_rewards):\n",
    "        batch_size = len(states)#.shape[]\n",
    "\n",
    "        for i in range(batch_size // self.mini_batch_size):\n",
    "            rand_ids = np.random.randint(0, batch_size, self.mini_batch_size)\n",
    "\n",
    "            yield torch.cat(states)[rand_ids], torch.cat(actions)[rand_ids], torch.cat(log_probs)[rand_ids], advantage[rand_ids], discounted_rewards[rand_ids]\n",
    "\n",
    "\n",
    "\n",
    "    def ppo_update(self, states, actions, log_probs, advantage, discounted_rewards, eps_clip=0.2):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        a_loss_batch = []\n",
    "        c_loss_batch = []\n",
    "        entropy_batch = []\n",
    "\n",
    "        for _ in range(self.ppo_epochs):\n",
    "            for states_i, old_actions, old_logprobs, advantage_i, discounted_reward_i  in self.ppo_iter(states, actions, log_probs, advantage, discounted_rewards):\n",
    "\n",
    "                self.c_optimizer.zero_grad()\n",
    "                #tran critic\n",
    "                new_value = self.critic(states_i.to(device))\n",
    "                c_loss = F.mse_loss(new_value, discounted_reward_i).cpu()\n",
    "                c_loss.backward(retain_graph=True)\n",
    "                clip_grad_norm_(self.critic.parameters(),self.clip_grad)\n",
    "                self.c_optimizer.step()\n",
    "\n",
    "                c_loss_batch.append(c_loss.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "                #train actor\n",
    "                self.a_optimizer.zero_grad()\n",
    "                _, _, dist = self.actor(states_i.to(device))\n",
    "                new_logprobs = dist.log_prob(old_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "                entropy_batch.append(entropy.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "                ratio = torch.exp(new_logprobs - old_logprobs.detach()).cpu()\n",
    "                surr = ratio * advantage_i.cpu()\n",
    "                clip = torch.clamp(ratio, 1.0 - eps_clip, 1.0 + eps_clip)\n",
    "\n",
    "                \n",
    "                a_loss  = - (torch.min(surr, clip * advantage_i.cpu() ).mean()) + self.entropy_beta * entropy.cpu()\n",
    "                a_loss.backward(retain_graph=True)\n",
    "                clip_grad_norm_(self.actor.parameters(),self.clip_grad)\n",
    "                self.a_optimizer.step()\n",
    "\n",
    "                a_loss_batch.append(a_loss.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "        return np.array(c_loss_batch).mean(), np.array(a_loss_batch).mean(), np.array(entropy_batch).mean()\n",
    "\n",
    "def list_to_tensor(list_):\n",
    "    return np.array(list(zip(*list_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "agent = Agent(state_size = state_size, action_size = action_size ,ppo_epochs = 5, mini_batch_size = 512,\\\n",
    "                 layer_size = 512 ,lr_a = 1e-4, lr_c = 1e-4, gamma = 0.99 , entropy_beta = 1e-4, clip_grad = 1)\n",
    "\n",
    "agent.actor.load_state_dict(torch.load(\"Crawler_weights/actor100.pth\"))\n",
    "agent.actor.eval()\n",
    "\n",
    "max_episodes = 0\n",
    "\n",
    "c_loss_list = []\n",
    "a_loss_list = []\n",
    "entropy_list = []\n",
    "\n",
    "\n",
    "average_100 = deque(maxlen = 100)\n",
    "\n",
    "mean_rewards = []\n",
    "max_rewards = []\n",
    "min_rewards = []\n",
    "average_100_rewards = []\n",
    "\n",
    "max_steps = 2024\n",
    "\n",
    "for ep in range(max_episodes+1):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    done = False\n",
    "    \n",
    "    states_batch = []\n",
    "    values_batch = []\n",
    "    actions_batch = []\n",
    "    logprobs_batch = []\n",
    "    rewards_batch = []\n",
    "    masks = []\n",
    "    scores = np.zeros(num_agents)\n",
    "    while True:\n",
    "\n",
    "        actions, logprobs  = agent.act(states)  \n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards\n",
    "        \n",
    "        states = next_states\n",
    "\n",
    "        if np.any(dones):\n",
    "            break\n",
    "\n",
    "    \n",
    "    mean_rewards.append(np.mean(scores))\n",
    "    min_rewards.append(np.min(scores))\n",
    "    max_rewards.append(np.max(scores))\n",
    "    average_100.append(np.mean(scores))\n",
    "    average_100_rewards.append(np.array(average_100).mean())\n",
    "    \n",
    "    print(\"\\rEpisode: {} | mean_reward: {:.2f} | min_reward: {:.2f} | max_reward: {:.2f} | Average_100: {:.2f}\".format(ep, np.mean(scores), np.min(scores), np.max(scores), np.mean(average_100)), end = \"\", flush = True)\n",
    "    if ep != 0 and ep % 100 == 0:\n",
    "        print(\"\\rEpisode: {} | mean_reward: {:.2f} | min_reward: {:.2f} | max_reward: {:.2f} | Average_100: {:.2f}\".format(ep, np.mean(scores), np.min(scores), np.max(scores), np.mean(average_100)))\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "env.close()\n",
    "# PLOTTING RESULTS\n",
    "\n",
    "plt.figure(figsize = (20,7))\n",
    "plt.subplot(1,4,1)\n",
    "plt.title(\"actor loss\")\n",
    "plt.plot(a_loss_list)\n",
    "plt.subplot(1,4,2)\n",
    "plt.title(\"critic loss\")\n",
    "plt.plot(c_loss_list)\n",
    "plt.subplot(1,4,3)\n",
    "plt.title(\"entropy\")\n",
    "plt.plot(entropy_list)\n",
    "plt.subplot(1,4,4)\n",
    "plt.title(\"rewards\")\n",
    "plt.plot(mean_rewards, c = \"b\")\n",
    "plt.plot(min_rewards, c = \"y\")\n",
    "plt.plot(max_rewards, c = \"r\")\n",
    "plt.plot(average_100_rewards, c = \"g\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
