# Double Deep Q_Learning with Experience Replay playing Cart Pole

[image1]: ./Imgs/test.png "Calculation Equation"
[image2]: ./Imgs/Q_table10000.png "Calculation Equation"
[image3]: ./Imgs/CNN_pong_converge.png

The difference between DQN and Double DQN is, that in Double DQN the target values get generated by a seperate neural network and not by the same that predicts the the Q_value as in DQN. 
[Paper](https://arxiv.org/abs/1509.06461)

### Learning Curve:

Learning Curve after 4000 Epochs and and exponentially epsilon Greedy

![alt text][image1]





### Youtube Video:
[Deep Q-Network plays Cart Pole](https://www.youtube.com/watch?v=9g2ZLPs5Rs0)

## Training to play pong with an Double Deep Q CNN
I trained a Double Deep Q-Network to play the Atari game Pong. After around 150000 frames it converged and beat its opponent constantly.  Thereby the convolutional neural network trained itself totally on visual inputs. Therefor the input images got converted to black-and-white and 4 images got stacked together so the network is able to recognize the velocity of the ball - which would be much more difficult to guess by only one state/image.  Also the network was trained offline with a memory technique called experienced replay and after each 1000 frames the target network was updated with the weights of the optimized model.

![alt text][image3]

### Youtube Video:
[Double Deep Q Network learns to play Pong](https://www.youtube.com/watch?v=I3dTyg_5rFc)
